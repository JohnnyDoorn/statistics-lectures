# Bayesian parameter estimation {.section}

## Updating belief

> Posterior $\propto$ Likelihood $\times$ Posterior

## So what is your belief

In lecture one we I tossed ten times. with a coin that was supposedly healed after hamering it straight. 

I arbitrarily assumed my $H_A: \theta=.25$.

Considering all possible values of $\theta$

$[0,1] \Rightarrow  \{\theta\in\Bbb R:0\le \theta\le 1\}$

## Draw your belief

```{r, echo=FALSE}
plot(.5,.5, 
     ylab = "likelihood",
     xlab = "Theta",
     ylim = c(0,1),
     xlim = c(0,1),
     type = "n")
```

## Prior distribution

You have assigned a prior probability distribution to the parameter $\theta$.

> This is your prior

Now we normally do not draw our priors, but we could. 

## Priors

We can choose a flat prior, or a beta distributed prior with different parameter values $a$ and $b$.

```{r}
theta = seq(0,1, .001)
layout(matrix(1:4,2,2))
plot(theta, dunif(theta),           type="l", ylab = "likelihood")
plot(theta, dbeta(theta, 3, 5),     type="l", ylab = "likelihood")
plot(theta, dbinom(25, 100, theta), type="l", ylab = "likelihood")
plot(theta, dbeta(theta, 2, 2),     type="l", ylab = "likelihood")
```

## Choose prior

Binomial distribution

$P^k (1-P)^{n-k}$
$P^25 (P-1)^{100-25}$

## Now what is the data saying

### My ten tosses

$$\begin{aligned}
  k &= 2 \\
  n &= 10
  \end{aligned}$$

```{r}
k = 2
n = 10
```

## Likelihood

How likely is the data assuming the data represents the parameter value:

$$\theta = \frac{2}{10} = `r k/n`$$
```{r}
theta.given.data = k/n

theta.given.data
```

## Likelihood function

How likely is 2 out of 10 for all possible $\theta$ values?

$P^k (1-P)^{n-k}$

```{r}
thetas = seq(0, 1, .01)

likelihood =  dbinom(k, n, thetas)

plot(thetas, dbinom(k, n, thetas),
     main = "Likelihood function",
     type='l', 
     ylab = "Likelihood", 
     las = 1)
abline(v=theta.given.data, lty='dashed')
```

-----

![](../../../../topics/bayes/likelihood_function.gif)

## Posterior

Now we can update our belief about the possible values of theta based on the data (the likelihood function) we found. For this we use Bayes rule.

$$\begin{aligned}
  {Posterior} &\propto {Likelihood} \times {Prior} \\
  P(\theta | D) &\propto P(D|\theta) \times P(\theta)
  \end{aligned}$$

## Theta all mighty

The true value of $theta$ for our binomial distribution.

$$\Huge \theta = .68$$

The data driver!
