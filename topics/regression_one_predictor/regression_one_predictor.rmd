# Regression (one predictor) {.section}

```{r, echo=FALSE}
if(!"DT" %in% installed.packages()) { install.packages("DT") }
library("DT")
```

```{r, echo=FALSE, eval=FALSE}
# Install if nesserary and load RCurl package
if(!'RCurl' %in% installed.packages()) { install.packages('RCurl') }
library(RCurl)

key = "1foUIwXqvvAWRg7FKdRMAIBLakpLbnhUpYhiTYayaPfc" # cijfers cijfer

# Set google url
google.url <- paste("https://docs.google.com/spreadsheets/d/",key,"/export?format=csv&id=",key, sep='')

# Load csv
myCsv <- getURL(google.url)
# Read csv and assing to data object
data <- read.csv(textConnection(myCsv))
# Change header names
names(data)[2:4] <- c('IQ', 'cijfers', 'betrouwbaar')

write.table(data, "cijfers.csv", row.names=FALSE, col.names=TRUE, dec='.')
```

## Regression {.smaller}

$$\LARGE{\text{outcome} = \text{model} + \text{error}}$$

In statistics, linear regression is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables denoted X. The case of one explanatory variable is called simple linear regression.

$$\LARGE{Y_i = \beta_0 + \beta_1 X_i + \epsilon_i}$$

In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data.

Source: [wikipedia](https://en.wikipedia.org/wiki/Linear_regression)

## Outcome vs Model

```{r, fig.align='center', fig.asp=.5}
error = c(2, 1, .5, .1)
n = 100

layout(matrix(1:4,1,4))
for(e in error) {
  
  x = rnorm(n)
  y = x + rnorm(n, 0 , e)
  
  r   = round(cor(x,y), 2)
  r.2 = round(r^2, 2)
  
  plot(x,y, las = 1, ylab = "outcome", xlab = "model", main = paste("r =", r," r2 = ", r.2), ylim=c(-2,2), xlim=c(-2,2))
  fit <- lm(y ~ x)
  abline(fit, col = "red")
  
}

```


## Assumptions

A selection from Field:

* Sensitivity
* Homoscedasticity

## Sensitivity

Outliers

* Extreme residuals
    * Cook's distance (< 1)
    * Mahalonobis (< 11 at N = 30)
    * Laverage (The average leverage value is defined as (k + 1)/n)
    
```{r, fig.align='center', fig.width=5, fig.height=5, echo=FALSE}
x[1] =  2
y[1] = -2

plot(x,y, 
     las  = 1, 
     ylab = "outcome", 
     xlab = "model", 
     main = paste("r =", r," r2 = ", r.2), 
     ylim = c(-2,2), 
     xlim = c(-2,2))
  
abline(fit, col = "red")

fit2 <- lm(y ~ x)
abline(fit2, col = "blue")
  
r   = round(cor(x,y), 2)
r.2 = round(r^2, 2)

text(x[1], y[1], paste("r =", r," r2 = ", r.2), pos = 2, col = "blue")
```

    
## Homoscedasticity

* Variance of residual should be equal across all expected values
     * Look at scatterplot of standardized: expected values $\times$ residuals. Roughly round shape is needed.

```{r, fig.align='center', fig.width=5, fig.height=5, echo=FALSE}
ZPRED  = scale(fit$fitted.values)
ZREDID = scale(fit$residuals)

plot(ZPRED, ZREDID)
abline(h = 0, v = 0, lwd=2)

#install.packages("plotrix")
if(!"plotrix" %in% installed.packages()) { install.packages("plotrix") };
library("plotrix")
draw.circle(0,0,1.7,col=rgb(1,0,0,.5))
```


## Simulation

```{r}
set.seed(28736)

N     = 123
mu    = 120
sigma = 15
IQ    = rnorm(N, mu, sigma)

b_0   =  1
b_1   = .04
error = rnorm(N, 0, 1)
  
grade = b_0 + b_1 * IQ + error

data = data.frame(grade, IQ)
data = round(data, 2)

# Write data for use in SPSS
write.table(data, "IQ.csv", row.names=FALSE, col.names=TRUE, dec='.')
```


## The data {.smaller}

```{r, echo=FALSE}
datatable(data, rownames = FALSE, options = list(searching = FALSE, scrollY = 415, paging = F, info = F))
```

## Calculate regression parameters {.subsection}

$${grade}_i = b_0 + b_1 {IQ}_i + \epsilon_i$$

```{r, eval=T}
IQ    = data$IQ
grade = data$grade
```

## Calculate $b_1$ {.subsection}

$$b_1 = r_{xy} \frac{s_y}{s_x}$$

```{r}
# Calculate b1

cor.grade.IQ = cor(grade,IQ)
sd.grade     = sd(grade)
sd.IQ        = sd(IQ)

b1 = cor.grade.IQ * ( sd.grade / sd.IQ )
b1
```

## Calculate $b_0$ {.subsection}

$$b_0 = \bar{y} - b_1 \bar{x}$$

```{r}
mean.grade = mean(grade)
mean.IQ    = mean(IQ)

b0 = mean.grade - b1 * mean.IQ
b0
```

## The slope

```{r, fig.align='center', fig.width=8, fig.height=6}
# Extra

plot(IQ,grade, xlim=c(0, 160), ylim=c(0,10))
abline(lm(grade~IQ))
abline(v=c(2,3),col='red')
abline(h=b0 + b1 * c(2, 3),col='red')

lines(c(3,3),c(b0 + b1 * 2,b0 + b1 * 3),col='green',lwd=3)
lines(c(2,3), rep(b0 + b1 * 2, 2),col='blue',lwd=3)

text(2.5, (b0 + b1 * 2),1, pos=1, cex=1)
text(3  , (b0 + b1 * 2.5),
     round((b0 + b1 * 3)-(b0 + b1 * 2), 2), 
     pos=4, 
     cex=1)
```

```{r, fig.align='center', fig.width=8, fig.height=6}
# Extra

plot(IQ,grade, xlim=c(1, 4), ylim=c(0.4,.6))
abline(lm(grade~IQ))
abline(v=c(2,3),col='red')
abline(h=b0 + b1 * c(2, 3),col='red')

lines(c(3,3),c(b0 + b1 * 2,b0 + b1 * 3),col='green',lwd=3)
lines(c(2,3), rep(b0 + b1 * 2, 2),col='blue',lwd=3)
text(2.5, (b0 + b1 * 2),1, pos=1, cex=1)
text(3  , (b0 + b1 * 2.5),
     round((b0 + b1 * 3)-(b0 + b1 * 2), 2), 
     pos=4, 
     cex=1)
```

## Calculate t-values for b's {.smaller .subsection}

$$t_{n-p-1} = \frac{b - \mu_b}{{SE}_b}$$

Where $n$ is the number of rows, $p$ is the number of predictors, $b$ is the beta coefficient and ${SE}_b$ its standard error.

```{r}
# Get Standard error's for b
fit <- lm(grade~IQ)
se = summary(fit)[4]$coefficients[,2]

se.b0 = se[1]
se.b1 = se[2]

cbind(se.b0, se.b1)
```

## {.smaller}

```{r}
# Calculate t's
mu.b0 = 0
mu.b1 = 0
t.b0  = (b0 - mu.b0) / se.b0; t.b0
t.b1  = (b1 - mu.b1) / se.b1; t.b1

n     = nrow(data) # number of rows
p     = 1          # number of predictors
df.b0 = n - p - 1
df.b1 = n - p - 1
```

## P-values of $b_0$

$$\begin{aligned}
t_{n-p-1} &= \frac{b - \mu_b}{{SE}_b} \\
df &= n - p - 1 \\
\end{aligned}$$

Where $b$ is het beta coeficient ${SE}$ is the standard error of the beta coefficient, $n$ is the number of subjects and $p$ the nubmer of predictors.

```{r}
if (!"visualize" %in% installed.packages()) { 
  install.packages("visualize")} 
  
library("visualize")

# p-value for b0
visualize.t(c(-t.b0,t.b0),df.b0,section='tails')
```

## P-values of $b_1$

```{r}
# p-value for b1
visualize.t(c(-t.b1,t.b1),df.b1,section='tails')
```

## Define regression equation

$$\widehat{grade} = {model} = b_0 + b_1 {IQ}$$

So now we can add the expected grade based on this model

```{r}
data$model = b0 + b1 * IQ
data$model <- round(data$model, 2)
```

## Expected values {.smaller .subsection}

Let's have a look

```{r, echo=FALSE}
datatable(data, rownames = FALSE, options = list(searching = FALSE, scrollY = 375, paging = F, info = F))
```

## $y$ vs $\hat{y}$

And lets have a look at this relation between expectation and reality

```{r, fig.height=5, fig.width=5, fig.align='center'}
model <- data$model

plot(model,grade, xlim = c(0,10), ylim = c(0,10))
lines(c(0,10), c(0,10), col='red')
```

## Error {.smaller}

The error / residual is the difference between the model expectation and reality

```{r}
data$error = grade - model
data$error <- round(data$error, 2)
```

```{r, echo=FALSE}
datatable(data, rownames = FALSE, options = list(searching = FALSE, scrollY = 300, paging = F, info = F))
```

## Model fit

The fit of the model is the amount of error, which can be viewed in the correlation ($r$).

```{r}
r = cor(model,grade)
r
```

## Explained variance {.subsection}

```{r}
r^2
```

## Explained variance visually

```{r, fig.align='center'}
# This is all the grade data
plot(grade,xlab='participants')
# With the mean
lines(1:n,rep(mean(grade),n),col='blue',lwd=2)

# The blue lines are the total variance, the deviation from the mean.
segments(1:n, grade, 1:n, mean(grade), col='blue')

# The model predicts the grade scores
points(1:n,model, col='red')

# The part of the variation that overlaps is the 'explained' variance. 
segments(1:n, model, 1:n, mean(grade), col='red')
```

The part that does not overlap is therefore 'unexplained' variance. And because $r^2$ is the explained variance, $1 - r^2$ is the unexplained variance.

## Test model fit

Compare model to mean Y (grade) as model

$$F = \frac{(n-p-1) r^2}{p (1-r^2)}$$

Where ${df}_{model} = n - p - 1 = N - K - 1$.

```{r}
F = ( (n-p-1)*r^2 ) / ( p*(1-r^2) )
F
```

## Signal to noise

Given the description of explained variance, F can again be seen as a proportion of explained to unexplained variance. Also known as a signal to noise ratio.

```{r}
df.model = p # n = rows, p = predictors
df.error = n-p-1

SS_model = sum((model - mean(grade))^2)
SS_error = sum((grade - model)^2)
MS_model = SS_model / df.model
MS_error = SS_error / df.error
F = MS_model / MS_error 
F
```

## {.smaller}

```{r}
SS_total = var(grade) * (n-1)
rbind(SS_total,
      SS_model,
      # Proportion explained variance
      SS_model / SS_total,
      r^2)
```

## Visualize

```{r, warning=FALSE}
visualize.f(F, df.model, df.error, section='upper')
```



